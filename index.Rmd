---
title: "Practical Machine Learning Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

This project uses the training dataset from the [Groupware@LES Weight Lifting Exercise][WLEData], which obtained data from test subjects performing dumbbell curls in different manners, and tries to predict the manner in which the dumbbell curl was performed. The Weight Lifting Exercise dataset was further split into training (75%) and test (25%) datasets, and a model was fitted to the training dataset and used on the test dataset to obtain an estimate for the out-of-sample error. A random forest algorithm with 2-fold cross-validation was used on relevant predictor variables in the training dataset to obtain a predictive model with an accuracy of 99% on the test dataset. This accuracy is similar to the reported accuracy of 98.03% obtained by the [original researchers][WLEData], although a true out-of-sample error rate might be more similar to the reported accuracy of 78.2% when the original researchers tried to apply the model to a test subject not included in the training data.

# Data and Methods

The training dataset from <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> contains data from six users doing 10 repetitions each of a dumbbell bicep curl in five different ways (quantified by the 'classe' variable). The data were collected from sensors (an accelerometer, a magnetometer, and a gyroscope) strapped to four areas: the user's belt, the user's arm, the user's forearm, and the dumbbell. There are 19,622 observations of 160 variables in this dataset.

The raw data have a non-trivial time-dependence, since observations made during one period of the exercise may more similar to each other than to observations made during another period of the exercise, independent of the way the user does the exercise. To minimize the effects of this time-dependence, the data were aggregated into time windows and statistics were collected on the raw data during each time window: the maximum value, minimum value, amplitude, average, standard deviation, and variance. However, if it is necessary to predict the way an exercise is being done given only a single observation, it should be better to use the raw data rather than the aggregated statistics cannot be used as a predictor

Since the test data we're trying to predict consists of 20 single observations (without the statistical information), the variables corresponding to statistics during a time window were discarded. Additionally, variables with Div/0 values were discarded in order to keep variables with no singular values. Finally, the variables for the timestamp, the time window, and the user's name were discarded, since we assume there's no time dependence and the model might be used to predict data from other users besides the six test subjects. 

The findCorrelation() function in the caret package was used to remove variables that are highly correlated with other variables. In the end, 47 variables including classe were retained. The 19622 observations were split into a training set (14718 observations) and a testing set (4904 observations). A random-forest algorithm with 2-fold cross-validation (chosen because it was a good compromise between accuracy and runtime) was trained on the training set and used to predict the classe variable in the testing set.


# Results

```{r setupCode, cache=TRUE, echo=FALSE}
library(dplyr)
library(caret)
library(doParallel)

# Read in the training dataset
dat <- read.csv("pml-training.csv")


# Remove all variables with NA values.
vars_to_keep <- NULL

for (n in 1:length(dat[1,])) {
  if (sum(is.na(dat[,n]))==0) {
    vars_to_keep <- c(vars_to_keep, n)
  }
}

stats_dat <- dat[,vars_to_keep]

# Remove all factor variables (usually caused by a NaN in the data).
vars_to_keep <- NULL

for (n in 1:length(stats_dat[1,])) {
  if (!is.factor(stats_dat[,n])) {
    vars_to_keep <- c(vars_to_keep, n)
  }
}

stats_dat <- stats_dat[,vars_to_keep]

# Find the correlation between variables and remove highly correlated variables
corr_mat <- abs(cor(stats_dat[,-length(names(stats_dat))]))
corr_indices <- findCorrelation(corr_mat, names = FALSE)
stats_dat <- stats_dat[, -corr_indices]

# Add the classe variable back
stats_dat$classe <- dat$classe

stats_dat <- stats_dat[,5:length(names(stats_dat))]

# Split data set into testing and training sets
set.seed(1523)
inTrain <- createDataPartition(y= stats_dat$classe, p= 0.75, list= FALSE)
training <- stats_dat[inTrain,]
testing <- stats_dat[-inTrain,]

# Make sure the model does k-fold cross-validation
set.seed(2678)
k <- 2
fitControl <- trainControl(method = "repeatedcv", number = k, repeats = k)

registerDoParallel()
#
rfFit <- train(classe~., method = "parRF", trControl = fitControl,tuneGrid=expand.grid(mtry = 2), data = training)
rfPred <- predict(rfFit, testing)
confusionMatrix(rfPred, testing$classe)
```
The expected out-of-sample accuracy of this model is 99%. However, this might still depend on using data obtained with the same subjects as the training dataset even though the user_name variable was excluded, since the training set had so few subjects.

[WLEData]: http://groupware.les.inf.puc-rio.br/har "Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013."